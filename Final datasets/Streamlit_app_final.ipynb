{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b0b0063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a920ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 14:03:40.532 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Anaconda_Python\\envs\\streamlit_app_env\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "C:\\Anaconda_Python\\envs\\streamlit_app_env\\lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.0 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "node array from the pickle has an incompatible dtype:\n- expected: {'names': ['left_child', 'right_child', 'feature', 'threshold', 'impurity', 'n_node_samples', 'weighted_n_node_samples', 'missing_go_to_left'], 'formats': ['<i8', '<i8', '<i8', '<f8', '<f8', '<i8', '<f8', 'u1'], 'offsets': [0, 8, 16, 24, 32, 40, 48, 56], 'itemsize': 64}\n- got     : [('left_child', '<i8'), ('right_child', '<i8'), ('feature', '<i8'), ('threshold', '<f8'), ('impurity', '<f8'), ('n_node_samples', '<i8'), ('weighted_n_node_samples', '<f8')]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 34\u001b[0m\n\u001b[0;32m     29\u001b[0m st\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOptimized Price for Maximum Revenue: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimized_price\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Load the trained Random Forest model\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrf_indoor_parking_model.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Prediction using the loaded model\u001b[39;00m\n\u001b[0;32m     39\u001b[0m selected_month_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mcolumns)  \u001b[38;5;66;03m# Extract features for the selected month\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda_Python\\envs\\streamlit_app_env\\lib\\site-packages\\joblib\\numpy_pickle.py:658\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[0;32m    656\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[1;32m--> 658\u001b[0m             obj \u001b[38;5;241m=\u001b[39m \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32mC:\\Anaconda_Python\\envs\\streamlit_app_env\\lib\\site-packages\\joblib\\numpy_pickle.py:577\u001b[0m, in \u001b[0;36m_unpickle\u001b[1;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[0;32m    575\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 577\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unpickler\u001b[38;5;241m.\u001b[39mcompat_mode:\n\u001b[0;32m    579\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    580\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib version less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease regenerate this pickle file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m                       \u001b[38;5;241m%\u001b[39m filename,\n\u001b[0;32m    583\u001b[0m                       \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda_Python\\envs\\streamlit_app_env\\lib\\pickle.py:1212\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1210\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[0;32m   1211\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[1;32m-> 1212\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[0;32m   1214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[1;32mC:\\Anaconda_Python\\envs\\streamlit_app_env\\lib\\site-packages\\joblib\\numpy_pickle.py:402\u001b[0m, in \u001b[0;36mNumpyUnpickler.load_build\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_build\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    395\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called to set the state of a newly created object.\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    We capture it to replace our place-holder objects, NDArrayWrapper or\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    NDArrayWrapper is used for backward compatibility with joblib <= 0.9.\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m     \u001b[43mUnpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_build\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;66;03m# For backward compatibility, we support NDArrayWrapper objects.\u001b[39;00m\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], (NDArrayWrapper, NumpyArrayWrapper)):\n",
      "File \u001b[1;32mC:\\Anaconda_Python\\envs\\streamlit_app_env\\lib\\pickle.py:1705\u001b[0m, in \u001b[0;36m_Unpickler.load_build\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1703\u001b[0m setstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(inst, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setstate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1705\u001b[0m     \u001b[43msetstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1707\u001b[0m slotstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32msklearn\\tree\\_tree.pyx:714\u001b[0m, in \u001b[0;36msklearn.tree._tree.Tree.__setstate__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msklearn\\tree\\_tree.pyx:1418\u001b[0m, in \u001b[0;36msklearn.tree._tree._check_node_ndarray\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: node array from the pickle has an incompatible dtype:\n- expected: {'names': ['left_child', 'right_child', 'feature', 'threshold', 'impurity', 'n_node_samples', 'weighted_n_node_samples', 'missing_go_to_left'], 'formats': ['<i8', '<i8', '<i8', '<f8', '<f8', '<i8', '<f8', 'u1'], 'offsets': [0, 8, 16, 24, 32, 40, 48, 56], 'itemsize': 64}\n- got     : [('left_child', '<i8'), ('right_child', '<i8'), ('feature', '<i8'), ('threshold', '<f8'), ('impurity', '<f8'), ('n_node_samples', '<i8'), ('weighted_n_node_samples', '<f8')]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyomo.environ import *\n",
    "import streamlit as st\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Existing Streamlit app code ...\n",
    "\n",
    "# Prediction Section\n",
    "st.subheader('Predicted Occupancy Percentage')\n",
    "selected_month = st.selectbox('Select Month for Prediction', ['May'])  # Add all months\n",
    "# Use the RandomForest model to predict for the selected month (This requires the model to be loaded and the data prepared)\n",
    "predicted_percentage = ...  # Placeholder for prediction code\n",
    "st.write(f'Predicted Occupancy Percentage for {selected_month}: {predicted_percentage}')\n",
    "\n",
    "# Optimization Section\n",
    "st.subheader('Price Optimization')\n",
    "price_upper_limit = st.number_input('Enter Price Upper Limit:', value=1000)  # Default value can be changed\n",
    "# Use the optimization code to get optimized price based on the constraint\n",
    "optimized_price = 1000  # Placeholder for optimization code\n",
    "st.write(f'Optimized Price for Maximum Revenue: {optimized_price}')\n",
    "\n",
    "\n",
    "\n",
    "# Load the trained Random Forest model\n",
    "rf_model = joblib.load('rf_indoor_parking_model.pkl')\n",
    "\n",
    "\n",
    "\n",
    "# Prediction using the loaded model\n",
    "selected_month_features = [0] * len(X_train.columns)  # Extract features for the selected month\n",
    "predicted_percentage = rf_model.predict([selected_month_features])[0]\n",
    "st.write(f'Predicted Occupancy Percentage for {selected_month}: {predicted_percentage:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "# Optimization Section\n",
    "st.subheader('Price Optimization')\n",
    "price_upper_limit = st.number_input('Enter Price Upper Limit:', value=1000)\n",
    "\n",
    "# Optimization code from \"Model_vIndoor_Parking.ipynb\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyomo.environ import *\n",
    "\n",
    "# Define the objective function\n",
    "def objective_rule(model):\n",
    "    return sum(df.loc[i, target_column] * model.price[i] for i in df.index)\n",
    "model.objective = Objective(rule=objective_rule, sense=maximize)\n",
    "\n",
    "# Define the objective function\n",
    "def objective_rule(model):\n",
    "    return sum(df.loc[i, target_column] * model.price[i] for i in df.index)\n",
    "model.objective = Objective(rule=objective_rule, sense=maximize)\n",
    "\n",
    "# Display results based on the optimization\n",
    "optimized_price = 1000  # Placeholder to extract optimized price from the optimization result\n",
    "st.write(f'Optimized Price for Maximum Revenue: {optimized_price}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5997ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_file_data_format(filename):\n",
    "    # Check if the file has .xlsx extension\n",
    "    if not filename.endswith('.xlsx'):\n",
    "        raise ValueError('Invalid file format. Please upload a .xlsx file.')\n",
    "        \n",
    "def validate_file_price_format(filename, valid_formats):\n",
    "    # Check if the file has a valid format\n",
    "    if not filename.lower().endswith(valid_formats):\n",
    "        raise ValueError(f'Invalid file format. Please upload a {valid_formats} file.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb3654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_file_data_name(filename):\n",
    "    # Extract the year and month from the file name\n",
    "    name_parts = os.path.splitext(filename)[0].split()\n",
    "    if len(name_parts) != 2:\n",
    "        raise ValueError('Invalid file name format. Please use \"year month\" format.')\n",
    "    year, month = name_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca91c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_file_price_name(filename, expected_name):\n",
    "    if filename.lower() != expected_name.lower():\n",
    "        raise ValueError(f'Invalid file name. Please upload a file named \"{expected_name}\", instead of \"{filename}\".')\n",
    "\n",
    "def validate_file_data_columns(df):\n",
    "    required_columns = ['Code', 'Description', 'Market Rent', 'Lease From', 'Lease To']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "    if missing_columns:\n",
    "        raise ValueError(f'The following columns are missing: {\", \".join(missing_columns)}')\n",
    "        \n",
    "def validate_worksheet_variables(worksheet, expected_variables):\n",
    "    # Check if the worksheet contains all the expected variables in the third row\n",
    "    header_row = 2  # Assuming the header row is the third row (index 2)\n",
    "    headers = worksheet.iloc[header_row].tolist()\n",
    "\n",
    "    for variable in expected_variables:\n",
    "        if variable not in headers:\n",
    "            raise ValueError(f'Variable \"{variable}\" not found in the worksheet.')\n",
    "\n",
    "\n",
    "def process_data_file(file):\n",
    "    validate_file_data_format(file.name)\n",
    "    validate_file_data_name(file.name)\n",
    "\n",
    "        \n",
    "    # Read the file into a DataFrame\n",
    "    df = pd.read_excel(file)\n",
    "\n",
    "    # Validate the columns in the DataFrame\n",
    "    validate_file_data_columns(df)\n",
    "\n",
    "    # Save the DataFrame as a CSV file with the same name as the original file\n",
    "    csv_filename = file.name.split()[0] + ' ' + file.name.split()[1][:3] + '.csv'\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    st.success(f'File successfully processed and saved as {csv_filename}.')\n",
    "\n",
    "def is_valid_year_format(input_str):\n",
    "    # Regular expression pattern to check if the input is a four-digit year\n",
    "    year_pattern = r'^\\d{4}$'\n",
    "    return bool(re.match(year_pattern, input_str))    \n",
    "    \n",
    "def process_price_file(file, expected_name,indooryear,outdooryear,storageyear):\n",
    "    \n",
    "    if not is_valid_year_format(indooryear) or not is_valid_year_format(outdooryear)or not is_valid_year_format(storageyear):\n",
    "        raise ValueError(\"Please enter valid year format for CSV file names.\")\n",
    "    \n",
    "    validate_file_price_format(file.name, '.xlsx')\n",
    "    validate_file_price_name(file.name, expected_name)\n",
    "    \n",
    "    \n",
    "    # Read the Excel file into a dictionary of DataFrames (one per sheet)\n",
    "    Parking_Rev = pd.read_excel(file,sheet_name='Parking Rev',skiprows=2)\n",
    "    Storage_Rev = pd.read_excel(file,sheet_name='Storage Rev',skiprows=1)\n",
    "    \n",
    "    # Check columns for required variables in Parking Rev sheet\n",
    "    expected_columns_parking = ['Average Garage', 'Average Surface']\n",
    "    missing_columns_parking = [col for col in expected_columns_parking if col not in Parking_Rev.columns]\n",
    "    if missing_columns_parking:\n",
    "        raise ValueError(f'Parking Rev sheet is missing the following columns: {\", \".join(missing_columns_parking)}')\n",
    "\n",
    "    # Check columns for required variables in Storage Rev sheet\n",
    "    expected_columns_storage = ['Average Charge']\n",
    "    missing_columns_storage = [col for col in expected_columns_storage if col not in Storage_Rev.columns]\n",
    "    if missing_columns_storage:\n",
    "        raise ValueError(f'Storage Rev sheet is missing the following columns: {\", \".join(missing_columns_storage)}')\n",
    "    \n",
    "    average_indoor_cols = [col for col in Parking_Rev.columns if \"Average Garage\" in col]\n",
    "    selected_cols1 = ['Property Code'] + average_indoor_cols\n",
    "    Indoor_comp = Parking_Rev[selected_cols1].copy()\n",
    "    for idx, col in enumerate(Indoor_comp.columns[1:], start=int(indooryear)):\n",
    "        year = str(idx)\n",
    "        Indoor_comp.rename(columns={col: f\"{year} Indoor Comp price\"}, inplace=True)\n",
    "        \n",
    "    average_outdoor_cols = [col for col in Parking_Rev.columns if \"Average Surface\" in col]\n",
    "    selected_cols2 = ['Property Code'] + average_outdoor_cols\n",
    "    outdoor_comp = Parking_Rev[selected_cols2].copy()\n",
    "    for idx, col in enumerate(outdoor_comp.columns[1:], start=int(outdooryear)):\n",
    "        year = str(idx)\n",
    "        outdoor_comp.rename(columns={col: f\"{year} Outdoor Comp price\"}, inplace=True)\n",
    "        \n",
    "    average_storage_cols = [col for col in Storage_Rev.columns if \"Average Charge\" in col]\n",
    "    selected_cols3 = ['Property Code'] + average_storage_cols\n",
    "    storage_comp = Storage_Rev[selected_cols3].copy()\n",
    "    for idx, col in enumerate(storage_comp.columns[1:], start=int(storageyear)):\n",
    "        year = str(idx)\n",
    "        storage_comp.rename(columns={col: f\"{year} Storage Comp price\"}, inplace=True)\n",
    "    # Save each sheet as a separate CSV file with the original sheet name\n",
    "    parking_rev_csv_filename = 'Parking Rev.csv'\n",
    "    storage_rev_csv_filename = 'Storage Rev.csv'\n",
    "    Parking_Rev.to_csv(parking_rev_csv_filename, index=False)\n",
    "    Storage_Rev.to_csv(storage_rev_csv_filename, index=False)\n",
    "    Indoor_comp.to_csv(\"Indoor Comp price.csv\", index=False)\n",
    "    outdoor_comp.to_csv(\"Outdoor Comp price.csv\", index=False)\n",
    "    storage_comp.to_csv(\"Storage Comp price.csv\", index=False)\n",
    "    st.success(f'Parking Rev sheet successfully saved as {parking_rev_csv_filename}.')\n",
    "    st.success(f'Storage Rev sheet successfully saved as {storage_rev_csv_filename}.')\n",
    "\n",
    "def generate_dataframe(file_path):\n",
    "    # Read CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "def generate_dataframe_name(file_name):\n",
    "    # Extract year and month from the file name\n",
    "    year, month = file_name.split()\n",
    "    dataframe_name = f\"{year}_{month}_data\"\n",
    "    return dataframe_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce78d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "   def combine_csv_files():\n",
    "    csv_files = [filename for filename in os.listdir() if re.match(r'\\d{4}\\s\\w+\\.csv', filename)]\n",
    "    csv_files = sorted(csv_files, key=lambda x: pd.Timestamp(re.findall(r'\\d{4}\\s\\w+', x)[0]))\n",
    "    \n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError('No CSV files found with the \"year_month\" format.')\n",
    "        \n",
    "    df_combined_indoor = pd.DataFrame()\n",
    "    df_combined_outdoor = pd.DataFrame()\n",
    "    df_combined_storage = pd.DataFrame()\n",
    "    \n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        df1 = pd.read_csv(csv_file)\n",
    "        df = df1.copy()\n",
    "        \n",
    "        nan_rows = df[df['Description'].isna()]\n",
    "    \n",
    "        # Iterate over the NaN rows and update the first column based on the string inside parentheses\n",
    "        for index, row in nan_rows.iterrows():\n",
    "            first_col_value = str(row.iloc[0])  # Convert the value of the first column to a string\n",
    "            match = re.search(r'\\((.*?)\\)', first_col_value)  # Extract the string inside parentheses\n",
    "    \n",
    "            if match:\n",
    "                extracted_string = match.group(1)  # Get the string inside parentheses\n",
    "                df.at[index, df.columns[0]] = extracted_string  # Update the value in the first column\n",
    "        df = df[df.iloc[:, 0] != 'Total']\n",
    "        df = df[df.iloc[:, 0] != 'Total for']\n",
    "        df = df[df.iloc[:, 0] != 'Start Here']\n",
    "        df = df[df.iloc[:, 0] != 'Start']\n",
    "    \n",
    "        df = df.dropna(subset=[df.columns[0]])\n",
    "    \n",
    "        # Reset the index after removing the rows\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "        df.dropna(how='all', inplace=True)\n",
    "        # Reset the index after removing the rows\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "        mask = df.iloc[:, 0].notnull() & df.iloc[:, 1:].isnull().all(axis=1)\n",
    "    \n",
    "        # Filter the DataFrame using the mask\n",
    "        filtered_df = df[mask]\n",
    "    \n",
    "        # Display the filtered DataFrame\n",
    "        first_time_index = filtered_df[~filtered_df.duplicated(keep='first')].index\n",
    "    \n",
    "        # Get the complete index list of the filtered DataFrame\n",
    "        complete_index = filtered_df.index\n",
    "    \n",
    "        # Get the index list excluding the first-time rows\n",
    "        index_list = complete_index.difference(first_time_index)\n",
    "    \n",
    "        df = df.drop(index_list)\n",
    "        mask = df.iloc[:, 0].notnull() & df.iloc[:, 1:].isnull().all(axis=1)\n",
    "        # Filter the DataFrame using the mask\n",
    "        filtered_df = df[mask]\n",
    "        #Getting the index of the couumnity\n",
    "        first_column_list = filtered_df.iloc[:, 0].tolist()\n",
    "        \n",
    "        file_date = re.findall(r'\\d{4}\\s\\w+', csv_file)[0]\n",
    "        date_obj = datetime.strptime(file_date, '%Y %b')\n",
    "        columns_to_keep = ['Description', 'Lease From', 'Lease To','Market Rent','Current Rent']\n",
    "        date_columns = ['Lease From', 'Lease To']  # Replace with your actual column names\n",
    "    \n",
    "    \n",
    "    \n",
    "        for i in range(len(first_column_list)):\n",
    "            if i < len(first_column_list) - 1:\n",
    "                a_index = df.index[df.iloc[:, 0] == first_column_list[i]].tolist()[0]\n",
    "                b_index = df.index[df.iloc[:, 0] == first_column_list[i+1]].tolist()[0]\n",
    "            else:\n",
    "                a_index = df.index[df.iloc[:, 0] == first_column_list[i]].tolist()[0]\n",
    "                b_index = df.shape[0]\n",
    "    \n",
    "    \n",
    "            dataframe_name = f\"{first_column_list[i]}_{date_obj.strftime('%Y%b')}\"\n",
    "    \n",
    "            new_dataframe = df.loc[a_index+1:b_index-1].copy()\n",
    "            new_dataframe.columns = df.columns  # Copy column names from the original DataFrame\n",
    "    \n",
    "            new_dataframe = new_dataframe[columns_to_keep]\n",
    "    \n",
    "            new_dataframe.fillna(\"None\", inplace=True)\n",
    "            new_dataframe.replace({np.nan: \"None\", pd.NaT: \"None\"}, inplace=True)\n",
    "            new_dataframe[date_columns] = new_dataframe[date_columns].apply(lambda x: pd.to_datetime(x, errors='coerce'))\n",
    "    \n",
    "            new_dataframe = new_dataframe[~new_dataframe['Description'].str.contains('Bike')]\n",
    "            new_dataframe.loc[new_dataframe['Description'].str.contains('Indoor|EV'), 'Description'] = 'Indoor Parking'\n",
    "            new_dataframe.loc[new_dataframe['Description'].str.contains('Outdoor'), 'Description'] = 'Outdoor Parking'\n",
    "            new_dataframe.loc[new_dataframe['Description'].str.contains('Storage'), 'Description'] = 'Storage'\n",
    "    \n",
    "            # mask = new_dataframe['Lease From'].notna() & new_dataframe['Lease To'].isna()\n",
    "            # new_dataframe.loc[mask, 'Lease To'] = pd.to_datetime('2099-01-01')\n",
    "            new_dataframe['Time_Difference'] = (date_obj - pd.to_datetime(new_dataframe['Lease From'])).dt.days / 30.4\n",
    "            new_dataframe['Move in advanced'] = new_dataframe['Time_Difference'].apply(lambda x: 1 if x < 0 else 0)\n",
    "            new_dataframe['Going to move in'] = new_dataframe['Time_Difference'].apply(lambda x: 1 if x < 0 and x > -1.5 else 0)\n",
    "            new_dataframe['Recent move in'] = new_dataframe['Time_Difference'].apply(lambda x: 1 if x <= 1 and x > 0 else 0)\n",
    "            new_dataframe = new_dataframe.drop('Time_Difference', axis=1)\n",
    "    \n",
    "            new_dataframe['Time_Difference'] = (pd.to_datetime(new_dataframe['Lease To'])-date_obj ).dt.days / 30.4\n",
    "            new_dataframe['Moving out'] = new_dataframe['Time_Difference'].apply(lambda x: 1 if x <= 3 else 0)\n",
    "            new_dataframe = new_dataframe.drop('Time_Difference', axis=1)\n",
    "    \n",
    "    \n",
    "            new_dataframe['Lease time'] = (pd.to_datetime(new_dataframe['Lease To']) - pd.to_datetime(new_dataframe['Lease From'])).dt.days / 30.4\n",
    "    \n",
    "            globals()[dataframe_name] = new_dataframe\n",
    "                \n",
    "        Final_df = pd.DataFrame(first_column_list, columns=['Property Code'])    \n",
    "            \n",
    "        for property_code in first_column_list:\n",
    "            # Get the dataframe name based on the property code\n",
    "            dataframe_name = f\"{property_code}_{date_obj.strftime('%Y%b')}\"\n",
    "        \n",
    "            # Load the corresponding dataframe using the dynamic variable name\n",
    "            df_temp = globals()[dataframe_name]\n",
    "        \n",
    "            # Separate dataframes for each description\n",
    "            df_indoor = df_temp[df_temp['Description'] == 'Indoor Parking']\n",
    "            df_outdoor = df_temp[df_temp['Description'] == 'Outdoor Parking']\n",
    "            df_storage = df_temp[df_temp['Description'] == 'Storage']\n",
    "            \n",
    "            # Calculate the count of \"Indoor Parking\"\n",
    "            count1_indoor = df_indoor.shape[0]  # Total Units\n",
    "            count2_indoor = df_indoor[(df_indoor['Lease time'] != 0) & (df_indoor['Lease time'].notna())].shape[0]\n",
    "            count4_indoor = df_indoor[df_indoor['Recent move in'] == 1].shape[0]\n",
    "            count5_indoor = df_indoor[df_indoor['Moving out'] == 1].shape[0]\n",
    "            count6_indoor_series = df_indoor[df_indoor['Market Rent'] != 0]['Market Rent'].mode()\n",
    "            count6_indoor = count6_indoor_series.iloc[0] if not count6_indoor_series.empty else 0\n",
    "            count7_indoor_series = df_indoor[df_indoor['Current Rent'] != 0]['Current Rent'].mean()\n",
    "            count7_indoor = count7_indoor_series#.iloc[0] #if not count7_indoor_series.empty else 0\n",
    "            if count1_indoor == 0:\n",
    "                count3_indoor = 0                \n",
    "                count8_indoor = 0\n",
    "                count9_indoor = 0\n",
    "            else:\n",
    "                count3_indoor = round((count2_indoor / count1_indoor) * 100, 2)\n",
    "                count8_indoor = round((count5_indoor / count1_indoor) * 100, 2)\n",
    "                count9_indoor = round((count6_indoor / count1_indoor) * 100, 2)\n",
    "    \n",
    "            count1_outdoor = df_outdoor.shape[0]  # Total Units\n",
    "            count2_outdoor = df_outdoor[(df_outdoor['Lease time'] != 0) & (df_outdoor['Lease time'].notna())].shape[0]\n",
    "            count4_outdoor = df_outdoor[df_outdoor['Recent move in'] == 1].shape[0]\n",
    "            count5_outdoor = df_outdoor[df_outdoor['Moving out'] == 1].shape[0]\n",
    "            count6_outdoor_series = df_outdoor[df_outdoor['Market Rent'] != 0]['Market Rent'].mode()\n",
    "            count6_outdoor = count6_outdoor_series.iloc[0] if not count6_outdoor_series.empty else 0\n",
    "            count7_outdoor_series = df_outdoor[df_outdoor['Current Rent'] != 0]['Current Rent'].mean()\n",
    "            count7_outdoor = count7_outdoor_series#.iloc[0] #if not count7_outdoor_series.empty else 0\n",
    "            if count1_outdoor == 0:\n",
    "                count3_outdoor = 0\n",
    "                count8_outdoor = 0\n",
    "                count9_outdoor = 0\n",
    "            else:\n",
    "                count3_outdoor = round((count2_outdoor / count1_outdoor) * 100, 2)\n",
    "                count8_outdoor = round((count5_outdoor / count1_outdoor) * 100, 2)\n",
    "                count9_outdoor = round((count6_outdoor / count1_outdoor) * 100, 2)\n",
    "            \n",
    "            count1_storage = df_storage.shape[0]  # Total Units\n",
    "            count2_storage = df_storage[(df_storage['Lease time'] != 0) & (df_storage['Lease time'].notna())].shape[0]\n",
    "            count4_storage = df_storage[df_storage['Recent move in'] == 1].shape[0]\n",
    "            count5_storage = df_storage[df_storage['Moving out'] == 1].shape[0]\n",
    "            count6_storage_series = df_storage[df_storage['Market Rent'] != 0]['Market Rent'].mode()\n",
    "            count6_storage = count6_storage_series.iloc[0] if not count6_storage_series.empty else 0\n",
    "            count7_storage_series = df_storage[df_storage['Current Rent'] != 0]['Current Rent'].mean()\n",
    "            count7_storage = count7_storage_series#.iloc[0] #if not count7_storage_series.empty else 0\n",
    "            if count1_storage == 0:\n",
    "                count3_storage = 0\n",
    "                count8_storage = 0\n",
    "                count9_storage = 0\n",
    "            else:\n",
    "                count3_storage = round((count2_storage / count1_storage) * 100, 2)\n",
    "                count8_storage = round((count5_storage / count1_storage) * 100, 2)\n",
    "                count9_storage = round((count6_storage / count1_storage) * 100, 2)  \n",
    "    \n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'Total Units ({date_obj.strftime(\"%Y%b\")}) (Indoor Parking)'] = count1_indoor\n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'Occupied ({date_obj.strftime(\"%Y%b\")}) (Indoor Parking)'] = count2_indoor\n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'Percentage% ({date_obj.strftime(\"%Y%b\")}) (Indoor Parking)'] = count3_indoor\n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'New Lease ({date_obj.strftime(\"%Y%b\")}) (Indoor Parking)'] = count4_indoor\n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'Ending Lease ({date_obj.strftime(\"%Y%b\")}) (Indoor Parking)'] = count5_indoor\n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'Market Price ({date_obj.strftime(\"%Y%b\")}) (Indoor Parking)'] = count6_indoor\n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'Current Price ({date_obj.strftime(\"%Y%b\")}) (Indoor Parking)'] = count7_indoor\n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'Lease_Turnover_Rate% ({date_obj.strftime(\"%Y%b\")}) (Indoor Parking)'] = count8_indoor\n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'Ending_Lease_Rate% ({date_obj.strftime(\"%Y%b\")}) (Indoor Parking)'] = count9_indoor\n",
    "            \n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'Total Units ({date_obj.strftime(\"%Y%b\")}) (Outdoor Parking)'] = count1_outdoor\n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'Occupied ({date_obj.strftime(\"%Y%b\")}) (Outdoor Parking)'] = count2_outdoor\n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'Percentage% ({date_obj.strftime(\"%Y%b\")}) (Outdoor Parking)'] = count3_outdoor\n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'New Lease ({date_obj.strftime(\"%Y%b\")}) (Outdoor Parking)'] = count4_outdoor\n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'Ending Lease ({date_obj.strftime(\"%Y%b\")}) (Outdoor Parking)'] = count5_outdoor\n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'Market Price ({date_obj.strftime(\"%Y%b\")}) (Outdoor Parking)'] = count6_outdoor\n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'Current Price ({date_obj.strftime(\"%Y%b\")}) (Outdoor Parking)'] = count7_outdoor\n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'Lease_Turnover_Rate% ({date_obj.strftime(\"%Y%b\")}) (Outdoor Parking)'] = count8_outdoor\n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'Ending_Lease_Rate% ({date_obj.strftime(\"%Y%b\")}) (Outdoor Parking)'] = count9_outdoor\n",
    "            \n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'Total Units ({date_obj.strftime(\"%Y%b\")}) (Storage)'] = count1_storage\n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'Occupied ({date_obj.strftime(\"%Y%b\")}) (Storage)'] = count2_storage\n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'Percentage% ({date_obj.strftime(\"%Y%b\")}) (Storage)'] = count3_storage\n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'New Lease ({date_obj.strftime(\"%Y%b\")}) (Storage)'] = count4_storage\n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'Ending Lease ({date_obj.strftime(\"%Y%b\")}) (Storage)'] = count5_storage\n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'Market Price ({date_obj.strftime(\"%Y%b\")}) (Storage)'] = count6_storage\n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'Current Price ({date_obj.strftime(\"%Y%b\")}) (Storage)'] = count7_storage\n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'Lease_Turnover_Rate% ({date_obj.strftime(\"%Y%b\")}) (Storage)'] = count8_storage\n",
    "            Final_df.loc[Final_df['Property Code'] == property_code, f'Ending_Lease_Rate% ({date_obj.strftime(\"%Y%b\")}) (Storage)'] = count9_storage\n",
    "            \n",
    "        df_indoor_parking = Final_df[['Property Code'] + [f'Total Units ({date_obj.strftime(\"%Y%b\")}) (Indoor Parking)',\n",
    "                                                         f'Occupied ({date_obj.strftime(\"%Y%b\")}) (Indoor Parking)',\n",
    "                                                         f'Percentage% ({date_obj.strftime(\"%Y%b\")}) (Indoor Parking)',\n",
    "                                                         f'New Lease ({date_obj.strftime(\"%Y%b\")}) (Indoor Parking)',\n",
    "                                                         f'Ending Lease ({date_obj.strftime(\"%Y%b\")}) (Indoor Parking)',\n",
    "                                                         f'Market Price ({date_obj.strftime(\"%Y%b\")}) (Indoor Parking)',\n",
    "                                                         f'Current Price ({date_obj.strftime(\"%Y%b\")}) (Indoor Parking)',\n",
    "                                                         f'Lease_Turnover_Rate% ({date_obj.strftime(\"%Y%b\")}) (Indoor Parking)',\n",
    "                                                         f'Ending_Lease_Rate% ({date_obj.strftime(\"%Y%b\")}) (Indoor Parking)']].copy()\n",
    "        \n",
    "        df_outdoor_parking = Final_df[['Property Code'] + [f'Total Units ({date_obj.strftime(\"%Y%b\")}) (Outdoor Parking)',\n",
    "                                                          f'Occupied ({date_obj.strftime(\"%Y%b\")}) (Outdoor Parking)',\n",
    "                                                          f'Percentage% ({date_obj.strftime(\"%Y%b\")}) (Outdoor Parking)',\n",
    "                                                          f'New Lease ({date_obj.strftime(\"%Y%b\")}) (Outdoor Parking)',\n",
    "                                                          f'Ending Lease ({date_obj.strftime(\"%Y%b\")}) (Outdoor Parking)',\n",
    "                                                          f'Market Price ({date_obj.strftime(\"%Y%b\")}) (Outdoor Parking)',\n",
    "                                                          f'Current Price ({date_obj.strftime(\"%Y%b\")}) (Outdoor Parking)',\n",
    "                                                          f'Lease_Turnover_Rate% ({date_obj.strftime(\"%Y%b\")}) (Outdoor Parking)',\n",
    "                                                          f'Ending_Lease_Rate% ({date_obj.strftime(\"%Y%b\")}) (Outdoor Parking)']].copy()\n",
    "        \n",
    "        df_storage = Final_df[['Property Code'] + [f'Total Units ({date_obj.strftime(\"%Y%b\")}) (Storage)',\n",
    "                                                   f'Occupied ({date_obj.strftime(\"%Y%b\")}) (Storage)',\n",
    "                                                   f'Percentage% ({date_obj.strftime(\"%Y%b\")}) (Storage)',\n",
    "                                                   f'New Lease ({date_obj.strftime(\"%Y%b\")}) (Storage)',\n",
    "                                                   f'Ending Lease ({date_obj.strftime(\"%Y%b\")}) (Storage)',\n",
    "                                                   f'Market Price ({date_obj.strftime(\"%Y%b\")}) (Storage)',\n",
    "                                                   f'Current Price ({date_obj.strftime(\"%Y%b\")}) (Storage)',\n",
    "                                                   f'Lease_Turnover_Rate% ({date_obj.strftime(\"%Y%b\")}) (Storage)',\n",
    "                                                   f'Ending_Lease_Rate% ({date_obj.strftime(\"%Y%b\")}) (Storage)']].copy()\n",
    "                \n",
    "                        # Concatenate dataframes to combined dataframes\n",
    "        df_combined_indoor = pd.concat([df_combined_indoor, df_indoor_parking], axis=1)\n",
    "        df_combined_outdoor = pd.concat([df_combined_outdoor, df_outdoor_parking], axis=1)\n",
    "        df_combined_storage = pd.concat([df_combined_storage, df_storage], axis=1)\n",
    "        \n",
    "        df_combined_indoor = df_combined_indoor.loc[:,~df_combined_indoor.columns.duplicated()].copy()\n",
    "        df_combined_outdoor = df_combined_outdoor.loc[:,~df_combined_outdoor.columns.duplicated()].copy()\n",
    "        df_combined_storage = df_combined_storage.loc[:,~df_combined_storage.columns.duplicated()].copy()  \n",
    "        \n",
    "        df_combined_indoor.fillna(0, inplace=True)\n",
    "        df_combined_outdoor.fillna(0, inplace=True)\n",
    "        df_combined_storage.fillna(0, inplace=True)\n",
    "        \n",
    "        Indoor_comp = pd.read_csv(\"Indoor Comp price.csv\")\n",
    "        outdoor_comp = pd.read_csv(\"Outdoor Comp price.csv\")\n",
    "        storage_comp = pd.read_csv(\"Storage Comp price.csv\")\n",
    "        \n",
    "        df_combined_indoor = pd.merge(Indoor_comp,df_combined_indoor,how=\"right\")\n",
    "        df_combined_outdoor = pd.merge(outdoor_comp,df_combined_outdoor,how=\"right\")\n",
    "        df_combined_storage = pd.merge(storage_comp,df_combined_storage,how=\"right\")\n",
    "        \n",
    "        # Check if the \"Final datasets\" folder exists, and create it if not\n",
    "        output_folder = \"Final datasets\"\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "    \n",
    "        indoor_parking_csv_file = os.path.join(output_folder, 'indoor_parking.csv')\n",
    "        outdoor_parking_csv_file = os.path.join(output_folder, 'outdoor_parking.csv')\n",
    "        storage_csv_file = os.path.join(output_folder, 'storage.csv')\n",
    "            \n",
    "        df_combined_indoor.to_csv(indoor_parking_csv_file, index=False)\n",
    "        df_combined_outdoor.to_csv(outdoor_parking_csv_file, index=False)\n",
    "        df_combined_storage.to_csv(storage_csv_file, index=False)\n",
    "        \n",
    "        st.success(\"CSV files saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d33861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlit app\n",
    "def main():\n",
    "    st.title(\"File Uploader\")\n",
    "\n",
    "    uploaded_file = st.file_uploader(\"Upload file(s)\", type=\"xlsx\", accept_multiple_files=True)\n",
    "    specific_file = st.file_uploader(\"Upload specific file\", type=\"xlsx\")\n",
    "\n",
    "    if uploaded_file is not None:\n",
    "        for file in uploaded_file:\n",
    "            try:\n",
    "                process_data_file(file)\n",
    "            except ValueError as e:\n",
    "                st.error(str(e))\n",
    "    \n",
    "    if specific_file is not None:\n",
    "        indooryear = st.text_input(\"What year is the first indoor parking comp price?\", \"\")\n",
    "        outdooryear = st.text_input(\"What year is the first outdoor parking comp price?\", \"\")\n",
    "        storageyear = st.text_input(\"What year is the first storage parking comp price?\", \"\")\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            process_price_file(specific_file, \"Parking Storage Rev Final v1.xlsx\",indooryear,outdooryear,storageyear)\n",
    "        except ValueError as e:\n",
    "            st.error(str(e))\n",
    "    \n",
    "    run_code = st.button(\"Run Code\")\n",
    "\n",
    "    if run_code:\n",
    "        # Get a list of all CSV files in the current directory\n",
    "        csv_files = glob.glob(\"*.csv\")\n",
    "\n",
    "        # Filter CSV files with the format \"year month\"\n",
    "        csv_files = [file for file in csv_files if len(file.split()) == 2]\n",
    "\n",
    "        if not csv_files:\n",
    "            st.write(\"No CSV files found with the format 'year month'.\")\n",
    "            return\n",
    "\n",
    "        for file_path in csv_files:\n",
    "            dataframe_name = generate_dataframe_name(os.path.splitext(file_path)[0])\n",
    "            df = generate_dataframe(file_path)\n",
    "\n",
    "            # Create a dataframe variable dynamically\n",
    "            globals()[dataframe_name] = df\n",
    "\n",
    "            st.subheader(f\"DataFrame: {dataframe_name}\")\n",
    "            st.write(df)\n",
    "\n",
    "        # Display the names of all the generated dataframes\n",
    "        st.subheader(\"Dataframe Names\")\n",
    "        dataframe_names = [name for name in globals() if name.endswith(\"_data\")]\n",
    "        st.write(dataframe_names)\n",
    "        \n",
    "    combine_button = st.button(\"Combine CSV Files\")\n",
    "    if combine_button:\n",
    "        combine_csv_files()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fe656e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
